# Cluster Image Numpy Files - Structure and Usage

## Overview
Cluster images are stored as numpy arrays (`.npy` files) optimized for neural network input. Each file represents a single cluster from one detector plane.

## File Format

### Dimensions
- **Shape**: `(128, 16)` - height × width
  - Height (Y-axis): 128 pixels = 128 time ticks
  - Width (X-axis): 16 pixels = 16 channels
- **Data type**: `float32`
- **File size**: ~8.2 KB per cluster

### Coordinate System
Following the display convention from `src/app/display.cpp`:
- **X-axis (columns, 0-15)**: Detector channels
  - Channels are mapped to consecutive indices (0, 1, 2, ...)
  - Original channel numbers are abstracted away
- **Y-axis (rows, 0-127)**: Time ticks
  - Corresponds to detector time ticks
  - Ticks are consecutive with no gaps

### Pixel Values
- **Range**: `[0.0, 1.0]` (normalized)
- **Meaning**: Normalized ADC intensity at each (channel, time) position
- **Encoding**: Pentagon interpolation matching `Display.cpp` behavior
  - Each TriggerPrimitive (TP) spans multiple time pixels (samples-over-threshold)
  - ADC values interpolated using pentagon shape: rise → peak → fall
  - Values normalized by log-scaling for better NN training

## Array Structure

```python
import numpy as np

# Load a cluster array
arr = np.load('cluster_planeX_0451.npy')

# Access pixel at channel=5, time=50
pixel_value = arr[50, 5]  # Note: arr[y, x] = arr[time, channel]

# Get dimensions
height, width = arr.shape  # (128, 16)

# Check sparsity
nonzero_pixels = np.count_nonzero(arr)
total_pixels = arr.size  # 2048
```

## Padding and Centering

Since most clusters are smaller than 16×128:
- Clusters are **centered** in the image
- **Padding** (zeros) added symmetrically around the cluster
- No artificial spacing between TPs - they remain consecutive

### Example
```
Original cluster: 4 channels × 80 ticks
Result in array:
  - X padding: (16 - 4) / 2 = 6 pixels on each side
  - Y padding: (128 - 80) / 2 = 24 pixels top/bottom
  - Cluster occupies pixels: [24:104, 6:10]
```

## Typical Statistics

From analysis of 2,741 clusters:
- **Mean nonzero pixels**: ~50-150 per cluster
- **Sparsity**: ~93-97% zeros (most pixels are padding/empty)
- **Channel extent**: 
  - Median: 2 channels
  - 95th percentile: 8 channels
- **Time extent**:
  - Median: 71 ticks
  - 95th percentile: 869 ticks (many exceed 128!)

## Handling Oversized Clusters

If a cluster exceeds 16×128:
- **Current behavior**: Cluster is cropped to fit
- **Width**: Clusters wider than 16 channels lose edge channels
- **Height**: Clusters taller than 128 ticks lose top/bottom ticks
- ~88% of clusters fit without cropping (based on width)
- ~30% fit in height (time dimension is challenging!)

**Note**: For clusters exceeding height=128, consider:
- Downsampling time axis (compress vertically)
- Using larger images (e.g., 16×256)
- Splitting very long clusters into multiple images

## File Naming Convention

```
cluster_plane{U,V,X}_{NNNN}.npy
```

- `plane{U,V,X}`: Detector plane (U, V, or X)
- `{NNNN}`: 4-digit cluster index (0000, 0001, ...)

### Examples
```
cluster_planeU_0000.npy  # First cluster from U plane
cluster_planeV_0042.npy  # 43rd cluster from V plane
cluster_planeX_0451.npy  # 452nd cluster from X plane
```

## Loading and Using in Python

### Basic Loading
```python
import numpy as np

# Load single cluster
arr = np.load('cluster_planeX_0451.npy')
print(f"Shape: {arr.shape}")  # (128, 16)
print(f"Type: {arr.dtype}")   # float32
print(f"Range: [{arr.min():.3f}, {arr.max():.3f}]")
```

### Batch Loading for NN Training
```python
import numpy as np
import glob

# Load all clusters from a folder
cluster_files = sorted(glob.glob('data/*/cluster_*.npy'))
clusters = np.array([np.load(f) for f in cluster_files])

# clusters.shape = (N, 128, 16) where N is number of clusters
print(f"Loaded {len(clusters)} clusters")

# For PyTorch: add channel dimension
# Input shape expected: (N, C, H, W) = (N, 1, 128, 16)
import torch
tensor = torch.from_numpy(clusters).unsqueeze(1)
```

### Visualization
```python
import numpy as np
import matplotlib.pyplot as plt

arr = np.load('cluster_planeX_0451.npy')

plt.figure(figsize=(6, 12))
plt.imshow(arr, aspect='auto', origin='lower', cmap='viridis')
plt.xlabel('Channel')
plt.ylabel('Time Tick')
plt.colorbar(label='Normalized ADC')
plt.title('Cluster Visualization')
plt.savefig('cluster.png', dpi=150, bbox_inches='tight')
```

## Memory Considerations

- **Single cluster**: 8.2 KB
- **1,000 clusters**: ~8 MB
- **10,000 clusters**: ~80 MB
- **Full dataset (2,741)**: ~22 MB

All clusters fit easily in RAM for batch processing.

## Generation Process

Clusters are generated by `scripts/generate_cluster_images.sh`:
1. Reads ROOT files with cluster data
2. For each cluster:
   - Extracts TriggerPrimitive (TP) data: channels, time, ADC
   - Maps channels to consecutive 0-15 indices
   - For each TP, fills time pixels using pentagon interpolation
   - Centers result in 16×128 array with padding
3. Normalizes values to [0, 1]
4. Saves as `.npy` file

## Related Tools

- **generate_cluster_images.sh**: Batch generator
- **show_single_cluster.py**: Visualize one cluster
- **view_cluster_arrays.py**: View random samples
- **verify_consecutive.py**: Verify structure and padding
- **analyze_cluster_sizes.py**: Analyze dimension statistics

## Pentagon Interpolation Details

Pentagon interpolation replicates `Display.cpp` behavior:

```
ADC
 ^
 |     /‾‾‾\
 |    /     \
 |   /       \
 |__/         \___
 +----------------> Time
   rise peak fall
```

Each TP contributes ADC values across its time extent:
- **Rise**: Linear increase from 0 to intermediate height
- **Peak**: Maximum ADC at samples-to-peak position
- **Fall**: Linear decrease back to 0
- **Duration**: samples-over-threshold (sot)

This creates realistic "pulse" shapes matching detector physics.
